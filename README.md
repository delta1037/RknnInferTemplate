# RknnInferTemplate
一个RKNN模型推理部署模板：将数据的输入输出（不同模型适配不同的输入源和做输出结果的处理）做成易于替换的插件的形式，将模型调度部分（通用部分）单独封装起来，减小模型部署工作量。

# 一、简介

在这里做通用模板的详细调研和设计。并且最近（2023-08-03）需要做毕设的调研，暂时也没有合适的人手来做通用模板的开发，所以我就先把设计写的清楚点，后续慢慢来做。

为什么使用NPU推理部署：板卡上包含了CPU算力和NPU算力，其中NPU算力仅可用于加速推理（作为边缘设备，推理应用更广泛一些）。于是为了充分使用板卡上的算力，可以考虑将CPU算力用于模型的训练（对于有需要在板卡上训练的场景来说）；将NPU算力用于模型的推理应用（一般放在边缘上的模型都会做推理，所以推理场景更广泛），而且将模型训练和模型推理使用的算力分开，也能让推理更顺畅（不会因为突如其来的模型训练占用算力导致不能推理，另外NPU的速度相对CPU来说更快一些，大约快了CPU的50%（来源于[网友的测试](https://zhuanlan.zhihu.com/p/529861266)））。

## 1.1 需求简介

有通用模板的需求是因为在6G项目中购置了一批板卡，后续将在板卡上做训练和运行模型；并且最近加了一个ROCK PI的开发群，群内大多都对模型的NPU推理部署有需求但是对C++和多线程不是很了解，于是有做一个通用模板来部署模型的想法。

ROCK PI 上使用NPU推理的模型需要做模型转换，转换后的模型也都基本是一致的，于是调用模型推理的方式也基本是一样的，所以可以将调度的部分封装起来做成一个模板。在经过在开发群的一段实际窥屏后，整理了对模板的以下要求：

- 模型推理调度功能：能够同时支持多个输入源，和多线程推理（这也是比较有挑战性的一点，需要考虑线程之间的协同工作）
- 模板对不同需求的适应性：不同的模型有不同的数据源，包括文本数据，图像数据，视频数据（实时的/非实时的），甚至语音数据，所以需要考虑不同类型输入的接入，已经异类输入的可扩展性。

# 二、使用模板

## 2.1 编写插件



